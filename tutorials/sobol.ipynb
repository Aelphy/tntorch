{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobol Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sobol's method* is one of the most popular for global sensitivity analysis. It builds on the [ANOVA decomposition](https://github.com/rballester/tntorch/blob/master/tutorials/anova.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10D TT tensor:\n",
       "\n",
       " 32  32  32  32  32  32  32  32  32  32\n",
       "  |   |   |   |   |   |   |   |   |   |\n",
       " (0) (1) (2) (3) (4) (5) (6) (7) (8) (9)\n",
       " / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ / \\\n",
       "1   5   5   5   5   5   5   5   5   5   1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tntorch as tn\n",
    "import torch\n",
    "\n",
    "N = 10\n",
    "t = tn.rand([32]*N, ranks_tt=5)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With *tntorch* we can handle all Sobol indices (i.e. for all subsets $\\alpha \\subseteq \\{0, \\dots, N-1\\}$) at once. We can access and aggregate them using the function `sobol()` with the appropriate mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Variables\n",
    "\n",
    "The effect attributable to one variable $n$ only (without interactions with others) is known as its *variance component* and denoted as $S_n$. Let's compute it for the first variable $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1824)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, z = tn.symbols(N)[:3]\n",
    "tn.sobol(t, mask=tn.only(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(see [this notebook](https://github.com/rballester/tntorch/blob/master/tutorials/logic.ipynb) for more on symbols and masks)\n",
    "\n",
    "Input parameters $x, y, \\dots$ need to be independently distributed. By default, uniform marginal distributions are assumed, but you can specify others with the `marginals` argument (list of vectors). For instance, if the first variable can take one value only, then its sensitivity indices will be 0 (no matter how strong its effect on the multidimensional model is!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marginals = [None]*N  # By default, None means uniform\n",
    "marginals[0] = torch.zeros(32)\n",
    "marginals[0][0] = 1  # The marginal PMF is all zeros but the first value\n",
    "tn.sobol(t, tn.only(x), marginals=marginals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect that also includes $x$'s interaction with other variables is called *total Sobol index* (it's always larger than the corresponding variance component):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2353)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn.sobol(t, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuples of variables\n",
    "\n",
    "What are the indices for the first and third variables $x$ and $z$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0032)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn.sobol(t, tn.only(x & z))  # Variance component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3226)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn.sobol(t, x | z)  # Total index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tuples of variables two additional kinds of indices exist. The *closed index* aggregates all components for tuples *included* in $\\alpha$, and for tuple $\\{x, z\\}$ it can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2546)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn.sobol(t, tn.only(x | z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *superset index* aggregates all components for tuples *that include* $\\alpha$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0054)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn.sobol(t, x & z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily count the influence of all $k$-plets of variables combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8509)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn.sobol(t, tn.weight_mask(N, weight=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we'll get the same result if we combine the effects differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3226)\n",
      "tensor(0.3226)\n"
     ]
    }
   ],
   "source": [
    "print(tn.sobol(t, x & ~z) + tn.sobol(t, ~x & z) + tn.sobol(t, x & z))\n",
    "print(tn.sobol(t, x) + tn.sobol(t, z) - tn.sobol(t, x & z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mean Dimension\n",
    "\n",
    "Variance components are the basis for an important advanced sensitivity metric, the [*mean dimension*](https://www.jstor.org/stable/27590729). It's defined as $D_S := \\sum_{\\alpha} |\\alpha| \\cdot S_{\\alpha}$ and computed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1681)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn.mean_dimension(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute it in one line by weighting the Sobol indices by their tuple weight (according to the definition of mean dimension):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1681)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn.sobol(t, tn.weight(t.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean dimension is always greater or equal than 1. It gives a notion of *complexity* of a multidimensional function (the lower the mean dimension, the simpler it is). For example, rounding a tensor usually results in a lower mean dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "errors = 10**np.linspace(-1, -3, 25)\n",
    "mean_dimensions = []\n",
    "for eps in errors:\n",
    "    mean_dimensions.append(tn.mean_dimension(tn.round(t, eps=eps)))\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(np.log10(errors), mean_dimensions)\n",
    "plt.xlabel('log10(rounding error)')\n",
    "plt.ylabel('Mean dimension')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dimension Distribution\n",
    "\n",
    "Last, the [*dimension distribution*](http://www3.stat.sinica.edu.tw/statistica/oldpdf/A13n11.pdf) gathers the relevance of $k$-tuples of variables for each $k = 1, \\dots, N$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn.dimension_distribution(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be viewed as a probability mass function, namely the probability of choosing a $k$-variable tuple, if tuples are chosen according to their variance components. The expected value of this random variable is the mean dimension. Naturally, the dimension distribution must sum to $1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(tn.dimension_distribution(t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
