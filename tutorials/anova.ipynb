{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *ANOVA decomposition* is defined for any multidimensional function $f: \\mathbb{R}^N \\to \\mathbb{R}$ whose inputs are independently distributed. It partitions the total variance of the model, $\\mathrm{Var}[f]$, as a sum of variances of orthogonal functions $\\mathrm{Var}[f_{\\alpha}]$ for all possible subsets $\\alpha$ of the input variables $\\{x_0, \\dots, x_{N-1}\\}$. Each $f_{\\alpha}$ depends effectively on the variables contained in $\\alpha$ only, and is constant with respect to the rest.\n",
    "\n",
    "Reference: [*\"Sobol Tensor Trains for Global Sensitivity Analysis\"*](https://arxiv.org/abs/1712.00233), R. Ballester-Ripoll, E. G. Paredes, R. Pajarola (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tntorch as tn\n",
    "\n",
    "N = 4\n",
    "t = tn.rand([32]*N, ranks_tt=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute all ANOVA terms in one single tensor network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4D TT-Tucker tensor:\n",
      "\n",
      " 33  33  33  33\n",
      "  |   |   |   |\n",
      " 32  32  32  32\n",
      " (0) (1) (2) (3)\n",
      " / \\ / \\ / \\ / \\\n",
      "1   5   5   5   1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anova = tn.anova_decomposition(t)\n",
    "print(anova)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tensor `anova` indexes *all* $2^N$ functions $f_{\\alpha}$ of the ANOVA decomposition of $f$, and we can access it using our [tensor masks](https://github.com/rballester/tntorch/blob/master/tutorials/logic.ipynb). For example, let's keep all terms that *do not* interact with $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z, w = tn.symbols(N)\n",
    "anova_cut = tn.mask(anova, ~w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can undo the decomposition to obtain a regular tensor again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cut = tn.undo_anova_decomposition(anova_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our truncated tensor `t_cut` has become constant with respect to the fourth variable $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.3290, 10.3290, 10.3290, 10.3290, 10.3290, 10.3290, 10.3290, 10.3290,\n",
       "        10.3290, 10.3290, 10.3290, 10.3290, 10.3290, 10.3290, 10.3290, 10.3290,\n",
       "        10.3290, 10.3290, 10.3290, 10.3290, 10.3290, 10.3290, 10.3290, 10.3290,\n",
       "        10.3290, 10.3290, 10.3290, 10.3290, 10.3290, 10.3290, 10.3290, 10.3290])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_cut[0, 0, 0, :].full()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much did we lose by making that variable unimportant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The truncated tensor accounts for 49.1902% of the original variance.\n"
     ]
    }
   ],
   "source": [
    "print('The truncated tensor accounts for {:g}% of the original variance.'.format(tn.var(t_cut) / tn.var(t) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which is also what [Sobol's method](https://github.com/rballester/tntorch/blob/master/tutorials/sobol.ipynb)) gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(49.1902)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn.sobol(t, ~w) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or, equivalently,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(49.1902)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn.sobol(t, tn.only(x | y | z)) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
